√ñneriler
Design and Calibration of a Multi-view TOF Sensor Fusion System

Bolum 4.1 In practice, we can discard these unreliable
measurements from recorded data by enforcing a
depth difference threshold around depth discontinuities.

Previous Works

TOF Sensor Calibraiton for a Color and Depth Camera Pair makalesinde 2.5 D Pattern Board kullanƒ±lmƒ±≈ü ve ray correction ve range bias correction uygulanmƒ±≈ütƒ±r. Range Error lar clusterlar la ifade edilip B spline ile hatalar g√∂sterilmi≈ütir. 
Bu makalede kullanƒ±lan bir tool su adrestedir: http://george-vogiatzis.org/calib/
Makale i√ßin yazƒ±lan yazƒ±lƒ±mlar ise sites.google.com/site/jyjungcv adresindedir

-----------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------
3D measurements from imaging laser radars: how good are they?
LIDAR lar ile ilgili 1992 model bir √ßalƒ±≈üma.√ñl√ß√ºm modelleri i√ßin ‚Äúrange ve intensity‚Äù bir karma≈üƒ±k sayƒ± olarak modellenmi≈ü.  Problem sebeplerinden birisi de mixed piksel effect denmi≈ü ve muhtemel √ß√∂z√ºmlerden biri ise median filter uygulamak olduƒüu belirtilmi≈ü.

-----------------------------------------------------------------------------------------------------
New Insights into the Calibration of ToF-Sensors
PMD lerden bahsedilmi≈ü..

For instance, the measured distance of a PMD
is affected by a systematic error [2, 5], the integration time
and the amount of incident active light [6].





-----------------------------------------------------------------------------------------------------
Cross-Calibration of Time-of-flight and Colour Cameras
Miles Hansard, Georgios Evangelidis, Quentin Pelorson, Radu Horaud

This paper presents a geometric framework for the resulting
multi-view and multi-modal calibration problem. It is shown that
three-dimensional projective transformations can be used to align depth and
parallax-based representations of the scene, with or without Euclidean reconstruction.
A new evaluation procedure is also developed; this allows the
reprojection error to be decomposed into calibration and sensor-dependent
components. The complete approach is demonstrated on a network of three
time-of-flight and six colour cameras.

-----------------------------------------------------------------------------------------------------
Distance errors correction for the Time of Flight (ToF) Cameras.

Abstract--One of the most important distance measurement errors is produced by light reflections. These errors can‚Äôt be avoided and black are more affected than white objects.
The measured distance by the ToF camera to an object in the scene changes if surrounding objects are moved. The distance error can be greater than 50% and camera calibration is useless if objects are moved. The calibration method we propose can be performed in any conditions not only in the laboratory. The distance errors for all objects in the scene can be corrected if on the objects are attached white or black tags/ labels. The ToF cameras can be improved using an active illumination with structured light. The improvement will eliminate the distance errors produced by light reflections.

Burada etraftan yansƒ±yan ƒ±≈üƒ±klarƒ±n etkisini azaltmak i√ßin kontrast ozellikli komsu pikseller arasƒ±nda kar≈üƒ±la≈ütƒ±rma yapƒ±lmƒ±≈ü.


---------------------------------------------------------------------
---------------------------------------------------------------------

Makale 32
Fusion of Time of Flight Camera Point Clouds

Bu makalede bundle adjustment ve ransac kullanƒ±larak extrinsic parametreler bununuyor ve nokta bulutlarƒ± arasƒ±ndaki ili≈ükiler bulunuyor.

In this paper, we investigate fusion methods for partially overlapping range images, aiming to address the issues of lateral
field of view extension (by combining depth images with parallel view axes) and occlusion removal (by imaging the same scene from different viewpoints).

In this section, we consider the problem of aligning two point clouds P0 and P1 acquired by TOF devices C0 and C1, in the specific case where the two fields of view overlap partially. More specifically, the alignment procedure should allow to obtain the rigid body transformation TC0;C1 between the point of view of cameras C0 and C1. This
rigid body transformation can be decomposed in a rotationRand a translation T, and is therefore fully specified by 6 parameters : the three translation coefficients in T, and the rotation angles _ (yaw), _ (pitch) and   (tilt). If those parameters are known, it becomes possible, for each point x1 in the cloud P1, to compute the corresponding position x0 in the coordinates system of C0



---------------------------------------------
--------------------------------------------

Makale 34

Projective Alignment of Range and Parallax Data

3D reconstruction ile range data ili≈ükilendirilmi≈ü. Karƒ±≈üƒ±k, √ßok form√ºll√º.

An approximately Euclidean representation of the visible scene can be obtained directly from a range, or ‚Äòtime-offlight‚Äô, camera. An uncalibrated binocular system, in contrast, gives only a projective reconstruction of the scene. This paper analyzes the geometric mapping between the two representations, without requiring an intermediate calibration of the binocular system. The mapping can be found by either of two new methods, one of which requires pointcorrespondences between the range and colour cameras, and one of which does not. It is shown that these methods can be used to reproject the range data into the binocular images, which makes it possible to associate highresolution colour and texture with each point in the Euclidean representation.


---------------------------------------------
--------------------------------------------

Makale 41


CALIBRATION OF A PMD-CAMERA USING A PLANAR CALIBRATION PATTERN
TOGETHER WITH A MULTI-CAMERA SETUP

3 rgb bir pmd kullanarak opencv ile kalibrasyon yapƒ±lmƒ±≈ü, sentezlenmi≈ü g√∂r√ºnt√ºler ile normal g√∂r√ºnt√ºler kar≈üƒ±la≈ütƒ±rƒ±lmƒ±≈ü. Bulunan parametrelerin birbirleri ile ilgili korelasyonlarƒ± incelenmi≈ü.

We discuss the joint calibration of novel 3D range cameras based on the time-of-flight principle with the Photonic Mixing Device (PMD) and standard 2D CCD cameras. Due to the small field-of-view (fov) and low pixel resolution, PMD-cameras are difficult to calibrate with traditional calibration methods. In addition, the 3D range data contains systematic errors that need to be compensated. Therefore, a calibration method is developed that can estimate full intrinsic calibration of the PMD-camera including optical lens distortions and systematic range errors, and is able to calibrate the external orientation together with multiple 2D cameras that are rigidly coupled to the PMD-camera. The calibration approach is based on a planar checkerboard pattern as calibration reference, viewed from multiple angles and distances. By combining the PMD-camera with standard CCD-cameras the internal camera parameters can be estimated more precisely and the limitations of the small fov can be overcome. Furthermore we use the additional cameras to calibrate the systematic depth measurement error of the PMD-camera. We show that the correlation between rotation and translation estimation is
significantly reduced with our method.


---------------------------------------------
--------------------------------------------

Makale 44

Fusion of Stereo Vision and Time-of-Flight Imaging for Improved 3D Estimation


Disparity‚Äôler kullanƒ±larak 3d fusion yapƒ±lmƒ±≈ü.

In this paper the fusion of two 3D estimation techniques is suggested: Stereo vision and Time-of-Flight (TOF) imaging. By converting the TOF-depth measurements to stereo disparities the correspondence between images from a fast TOF-camera and standard high resolution camera pair are found so the TOF depth measurements can be linked to the image pairs. Also in the same framework a method is developed to initialize and constrain a hierarchical stereo matching algorithm. It is shown that in this way higher spatial resolution is obtained than by only using the TOF camera and higher quality dense stereo disparity maps are the results of this sensor fusion.


---------------------------------------------
--------------------------------------------

Makale 45

Fusion of Stereo-Camera and PMD-Camera Data for Real-
Time Suited Precise 3D Environment Reconstruction

ƒ∞ki kamera ve depth sensorlu sistem. √ñnce hepsi ayrƒ± ayrƒ± kalibre edilmi≈ü sonra da f√ºzyon yapƒ±lmƒ±≈ü. TOF calibration yapƒ±lƒ±rken phase value farklƒ± uzaklƒ±klar icin bulunarak kestirilmi≈ü.

Abstract - 3D environment reconstruction is a basic task, delivering the data for mapping, localization and navigation in mobile robotics. We present a new technique that combines a
stereo-camera system with a PMD-camera. Both systems generate distance images of the environment but with different characteristics. It is shown that each system compensates
effectively for the deficiencies of the other one. The combined system is real-time suited. Experimental data of an indoor scene including the calibration procedure are reported.






---------------------------------------------
--------------------------------------------

Makale 47

A Combined Approach for Estimating Patchlets from PMD Depth Images and Stereo Intensity Images

Fazla form√ºl, az detay var.

Abstract. Real-time active 3D range cameras based on time-of-flight technology using the Photonic Mixer Device (PMD) can be considered as a complementary technique for stereo-vision based depth estimation. Since those systems directly yield 3D measurements, they can also be used for initializing vision based approaches, especially in highly dynamic environments. Fusion of PMD depth images with passive intensity-based stereo is a promising approach for obtaining reliable surface reconstructions even in weakly textured surface regions. In this work a PMD-stereo fusion algorithm for the estimation of patchlets from a combined PMD-stereo camera rig will be presented. As patchlet we define an oriented small planar 3d patch with associated surface normal. Least-squares estimation schemes for estimating patchlets from PMD range images as well as from a pair of stereo images are derived. It is shown, how those two approaches can be fused into one single estimation,
that yields results even if either of the two single approaches fails.

The phase difference is measured by cross correlation between the sent and
received modulated signal by the PMD chip. Since the resolution of the phase
difference measurement is independent from distance, the achievable depth resolution
is independent from scene depth. This is in contrast to stereo triangulation
where depth accuracy is proportional to inverse depth.

---------------------------------------------
--------------------------------------------

Makale 48


NON-PARAMETRIC DEPTH CALIBRATION OF A TOF CAMERA


Fonksiyonel bir yakla≈üƒ±m, g√ºzel, makalede kullanƒ±labilir.

Time-of-Flight (TOF) cameras measure, in real-time, the distance between the camera and objects in the scene. This opens new perspectives in different application fields: 3D reconstruction, Augmented Reality, video-surveillance, etc. However, like any sensor, TOF cameras have limitations related to their technology. One of them is distance distortion. In this paper, we present a new depth calibration method (estimation of distance distortion) for TOF cameras. Our approach has several advantages. First, it is based on a non-parametric model, contrary to most of the other methods. Second, it models under the same formalism the distortion variation  according to the distance and the pixel position in the image. This improves calibration accuracy even at the image boundaries which are typically more distorted than the image center. A comparison with two state of the art parametric methods is presented.

‚ÄÉ
---------------------------------------------
--------------------------------------------

Makale 55

A Novel 2.5D Pattern for Extrinsic Calibration of ToF and Camera
Fusion System

Recently, many researchers have made efforts for accurate calibration of a Time-of-Flight camera to fully utilize
its provided depth values. Yet most previous works focus mainly on intrinsic calibration by modeling its systematic errors and noises while extrinsic calibration is also an important factor when constructing sensor fusion system. In this paper, we
present a calibration process that can correctly transfer the depth measurements onto the color image. We use 2.5D pattern
so that sufficient reprojection error can be considered for both color and ToF cameras. The issues on obtaining the
correct correspondences for this pattern are discussed. In the optimization stage, the depth constraint is also employed to
ensure the depth measurements to lie on the pattern plane. The strengths of the proposed method over previous approaches are evaluated in several robotic applications which require precise ToF and camera calibration.


The main reason that the traditional calibration methods
do not work well on calibrating camera-ToF sensor fusion
system is the characteristics of the ToF sensor. Unlike a 2D
laser range finder, the depth measurement that a typical 3D
ToF camera provides is inaccurate, and the amplitude images


*********************************************************************

‚Ä¢	Azure Kinect devices are calibrated with Brown Conrady which is compatible with OpenCV.

********************************

---------------------------------------------
--------------------------------------------

Makale 37

Revisiting Uncertainty Analysis for Optimum Planes Extracted from 3D Range Sensor Point-Clouds

Bol form√ºll√º, plain detection konulu bir √ßalƒ±≈üma.


Abstract‚ÄîIn this work, we utilize a recently studied more accurate range noise model for 3D sensors to derive from scratch the expressions
for the optimum plane which best fits a point-cloud and for the combined covariance matrix of the plane‚Äôs parameters. The parameters in question are the plane‚Äôs normal and its distance to the origin. The range standarddeviation model used by us is a quadratic function of the true range and is  a function of the incidence angle as well. We show that for this model, the maximum-likelihood plane is biased, whereas the least-squares plane is not. The plane-parameters‚Äô covariance matrix for the least-squares plane is shown to possess a number of desirable properties, e.g., the optimal solution forms its null-space and its components are functions of easily understood terms like the planar-patch‚Äôs center and scatter.We verify our covariance expression with that obtained by the eigenvector perturbation method. We further compare our method to that of renormalization with respect to the theoretically best covariance matrix in simulation. The application of our approach to real-time range-image registration and plane fusion is shown by an example using a commercially available 3D range sensor. Results show that our method has good accuracy, is fast to compute, and is easy to interpret intuitively.

Index Terms‚ÄîPlane-fitting, Plane uncertainty estimation, 3D Mapping, Plane

GENERALIZATION of traditional occupancy-grid 2D maps to three dimensions is hindered by the disproportionate increase in storage and computation requirements.

Given a set of noisy points known or hypothized to lie on a plane, the ‚Äúoptimum‚Äù plane can be extracted from them using methods surveyed in [9]. The main methods are that of least-squares [5], [9] and renormalization.



---------------------------------------------
--------------------------------------------

Makale 3

Technical Foundation and Calibration Methods for Time-of-Flight Cameras

Damien Lefloch1, Rahul Nair2,3, Frank Lenzen2,3, Henrik Schafer2,3, Lee
Streeter4, Michael J. Cree4, Reinhard Koch5 and Andreas Kolb

Technical Foundation and Calibration methods for TOF cams -- Leoch, Nair, Lenzen, Schafer, Koch

makalesinden alƒ±ntƒ±lar: 

ToF sensors usually provide two measurement frames at the same time from data acquired by the same pixel array; the depth and amplitude images. The amplitude image corresponds
to the amount of returning active light signal and is also considered a strong indicator of quality/ reliability of measurements.

Most of the ToF manufacturers built-in the following principle in their cameras such as pmdtechnologies 1, Mesa Imaging 2 or Soft Kinetic 3 (cf. Fig. 1). These cameras are able to retrieve 2:5D image at a frame rate of 30FPS; pmdtechnologies is currently working on faster device (such as the Camboard Nano) which operates at 90FPS. Note that common ToF 
cameras usually use high modulation frequency range that make them suitable for near or middle range applications.


The continuous modulation principle, also known as a continuous wave intensity modulation (CWIM) [21], is based on the correlation of the emitted signal o shifted by an oset 
phase and the incident signal r resulting from the reection of the modulated active illumination (NIRlight) by the observed scene. CWIM is used to estimate the distance between 
the target (i.e. observed objects) and the source of the active illumination (i.e. the camera). CWIM ToF sensors directly implement the correlation function on chip, composed of 
what is known in the literature as smart pixels [21].

21 lange'nin doktora tezi

Both emitted and incident signal can be expressed as a cosinusoidal function

It is understood that in this simple depth retrieval calculation from the phase shift, , simplications are made which leads to possible measurement errors, e.g the assumption 
that the active illumination module and the ToF sensors are placed in the same position in space; which is physically impossible.


Conversely, pulse modulation is an alternative tof principle which generates pulse of light of known dimension coupled with a fast shutter observation.
Pulse modulation az kullanƒ±lƒ±yor..

Beside integration time, that directly influences the signal-to-noise ratio (SNR) of the measurement and consequently the variance of the measured distance, the user can in
uenced the quality of the measurements made by setting the fm value to t the application. As stated by Lange [21], as fm increases the depth resolution increases but the non ambiguity range decreases.

wiggling hatalarƒ±ndan bahsedilmeli
systematic error
Use of the correlation of the physical light source with the formulas 1.1 lead to a periodic "wiggling" error which causes the calculated depth to oscillate around the actual 

depth. The actual form of this oscillation depends on the strength and frequencies of the higher order harmonics.


There are two approaches for solving this problem. The rst approach is to sample the correlation more phase shifts and extend the formulas to incorporate higher order harmonics

[8]. With current 2-tap sensor this approach induces more errors when observing dynamic scenes. The second approach which we will further discuss in 3.2 is to keep the formulas as they are and estimate the residual error between true and calculated depth [25, 38] . The residual can then be used in a calibration step to eliminate the error. Finally, [34] employ a phase modulation of the amplitude signal to attenuate the higher harmonics in the emitted amplitude.

2.2 Intensity-related Distance Error
In addition to the wiggling systematic error, the measured distance is greatly altered by an error dependent of the total amount of incident light recieved by the sensor. Measured distance of lower reflectivity objects appear closer to the camera (up to 3cm drift for the darkest objects). Fig. 3 highlights this error eect using a simple black-and-white checkerboard pattern. The described error is usually known as Intensity-related distance error and its cause is not fully understood yet [24].

**Depth Inhomogeneity**
An important type of errors in ToF imaging, the so-called flying pixels, occurs along depth inhomogeneities. To illustrate these errors, we consider a depth boundary with one 
foreground and one background object. In the case that the solid angle extent of a sensor pixel falls on the boundary of the foreground and the background, the recorded signal is a mixture of the light returns from both areas

**motion artifact hatalarƒ±**
Since these (pairs) of additional exposures have to be made sequentially, dynamic scenes lead to erroneous distance values at depth and reectivity boundaries.

**Multiple Returns**
Multipath interference can also occur intra-camera due to the light refraction and reection of an imaging lens and aperture [43, 3, 37]. The aperture eect is due to diraction 
which leads to localised blurring in the image formation process. Fine detail beyond the limits in angular resolution is greatly reduced, causing sharp edges to blur.

**Other error sources**
The most important error source in the sensor is a result of the photon counting process in the sensor. Since photons are detected only by a certain probability, Poisson noise is introduced.

The recorded light intensity in the raw channels is stemming from both active and background illumination. Isolating the active part of the signal reduces the SNR. Such a 
reduction could be compensated by increasing the integration time, which on the other hand increases the risk of an over-saturation of the sensor cells, leading to false depth 
estimation

Due to these short-comings current ToF cameras have a resolution smaller than half VGA, which is rather small in comparison to standard RGB or grayscale cameras.

**Depth Calibration**
It should be noted here, that since the ToF camera measures the time of ight along the light path of course, error calibration should be done with respect to the radial distance as well, not in Cartesian coordinates. One of the rst contributions to this topic is [25] by Lindner and Kolb. They combined a pixelwise linear calibration with a global B-splines t. In [38] Schiller et. al. used a polynomial to model the distance deviation.

** temperature vurgulanmalƒ±"
However, the calibration is only valid for the camera temperature it was recorded at, since the behavior changes with the temperature

4 Post-Processing Data Correction
additional errors are usually scene dependent (as dynamic environment), a last processing needs to be applied after the depth correction via calibration

4.1 Depth Inhomogeneity and Denoising
On methods applied to the 2D data we remark that median ltering is a simple and ecient means to for a rough correction of flying pixels, which are outside the objects' depth range

4.2 Motion Compensation
motion artifacts occur in dynamic scenes at depth and reectivity boundaries due to the sequential sampling of the correlation function. There are three (or arguably two)
dierent approaches to reduce such artifacts. One way is by decreasing the number of frames obtained sequentially and needed to produce a valid depth reconstruction. Another
approach commonly employed is composed of a detection step, where erroneous regions due to motion are found, followed by a correction step.

4.3 Multiple Return Correction
The determination of the multiple returns of multipath or mixed pixels essentially is the separation of complex phasors into two or more components.





